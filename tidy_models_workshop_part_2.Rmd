---
title: "Tidy Models Workshop Part Two"
author: "Jose Sotelo"
date: "10/6/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = F,message = F)
```

# **Welcome Back Everyone!**

**The goal of today is recap what we learned on Tuesday and further our understanding of Tidymodels**

Again, there is a lot of possible information that can be provided on this topic and this workshop isn't meant to exhaust those resources. If you have questions throughout the workshop you can type them in the chat or I will be happy to meet with you afterwords. If you would like some help using Tidymodels for your own data, we do free [consults](https://services.northwestern.edu/TDClient/30/Portal/Requests/ServiceDet?ID=93) for all of your data science needs!

## **Agenda for today**

-   Recap of what we went over last Tuesday

-   Set up a simple Classification model

    -   Prep data
    -   Create recipe
    -   "Bake"
    -   Fitting test data
    -   Assessing fit

-   Try Other types of Classification Models

    -   Experiment with different tuning parameters

-   If we have time, we will go over these:

    -   Talk about tuning grids
    -   Talk about different forms of resampling

## **Review**

The `tidymodels` framework is a collection of packages for modeling and machine learning using tidyverse principles.

![](images/tidymodels_packages.png)

### **We are Focused on Predictive Models**

Whereas **inferential models** help researchers understand the relationship between a set of predictors and an outcome, **predictive models** focus more on optimizing the predictive accuracy of the model.

### **Today We are Building Supervised Classification Models**

**Supervised models** are those that have an outcome variable. Linear regression, decision trees, and numerous other methodologies fall into this category.

Last week, we built **regression** models, which predicts numeric outcome.

This week, we focus on **classification**, which predicts an outcome that is an *ordered* or *unordered* set of qualitative values. A mode of transportation (car, airplane, boat, etc) would be this type of outcome.

### **Reminder of Basic Tidymodels Process**

1.  **Split the data** into `testing` and `training` set. We can either randomly split from the entire dataset or **stratify** based on some variable and then split.
2.  **Define a process for preparing a dataset** by defining a `recipe`. This can be used for modeling or stand-alone preprocessing.
3.  **Choose and define the model** you would like to use.
4.  **Combine your model and recipe** into one `workflow`.
5.  **Fit workflow** with your training data.
6.  Use the trained workflow to **predict the unseen test data**.
7.  **Evaluate performance** of the model with performance metrics.

![](images/tidy_models_basics.png)

## **Load Packages and Set Seed**

Following code installs any packages for this workshop that doesn't exist in your system and then load them.

```{r,warning=FALSE,message=FALSE}

# install any packages that doesn't exist in the system
workshop_pkgs <- c(
  "tidymodels", "tidyverse", "janitor", "skimr", "vip", 
  "yardstick", "ranger", "glmnet", "mlbench", "nnet",
  "gridExtra"
)
missing_pkgs <- workshop_pkgs[!(workshop_pkgs %in% installed.packages()[,"Package"])]
if (length(missing_pkgs) != 0) {
  install.packages(missing_pkgs)
}

# Load packages here!
library(tidymodels)
library(tidyverse)
library(janitor)
library(skimr)
library(vip)
library(yardstick)
library(ranger)
library(glmnet)
library(mlbench)
library(nnet)
library(gridExtra)

# Set seed here!
set.seed(1192)
```

# **Classification Model**

For our classification model, where our outcomes are qualititative, we will use the titanic data. Our goal is to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).

Let's Load the data from `data/titanic.csv` into *R* and familiarize ourselves with the variables it contains using the codebook (`data/titanic_codebook.txt`).

```{r}
# read titanic data and clean the column names
titanic <- read_csv(file="data/titanic.csv") %>%
  clean_names()
print(head(titanic))
```

According to the codebook, we can see that `survival`, `pclass`, and `sex` are variables we would like to include in the model that are categorical.

We want to convert them to `factor` data type. Benefit of doing this is that R will know which variable is categorical vs numeric and we can explicitly choose which category serves as baseline. We want to reorder the factor so that `"Yes"` is baseline for \` and `"female"` is baseline for `sex` - just specify that category first when defining levels.

```{r}
# convert categorical to factor
titanic <- titanic %>%
  mutate(
    pclass = factor(pclass),
    survived = factor(survived,levels=c("Yes","No")),
    sex = factor(sex, levels = c("female", "male"))
  )
```

## **Look at the Distribution of our Outcome Variable**

Using the full data set, explore/describe the distribution of the outcome variable `survived`.

Perform a skim of the training data and note any potential issues such as missingness.

```{r}

ggplot(titanic,aes(survived)) +
  geom_bar() +
  theme_minimal() + 
  ggtitle("Distribution of Survival Outcomes")

skim_without_charts(titanic)

```

`cabin` and `embarked` variables have some missing values, but we won't be using those variables so we're don't have to be worried about that. Do note that there are 177 observations with missing `age` variable. We will come back to this later and try to fill in these values by using imputation methods.

## **Split our data**

Let's use **stratified sampling**. Stratified sample splits the data within the stratification variable. Last week, we stratified based on age, which stratified the data set based on quartiles because age is a numeric variable. This time, we stratify based on survival, which is a categorical variable.

Why is it a good idea to use stratified sampling for this kind of data? This ensures that the distribution of survival outcome remains same between the training and testing data set. This is particularly useful if a certain category is underrepresented in the data set.

We should also choose the proportions to split the data into. Let's verify that the training and testing data sets have the appropriate number of observations.

```{r}
titanic_split <- initial_split(titanic,prop = .70, strata = survived)

titanic_split

titanic_test <- testing(titanic_split)

titanic_train <- training(titanic_split)
```

```{r}
g1 <- ggplot(titanic_train, aes(survived)) +
  geom_bar() +
  theme_minimal() +
  ggtitle("Survival Distribution in\nTraining Set")
g2 <- ggplot(titanic_test, aes(survived)) +
  geom_bar() +
  theme_minimal() +
  ggtitle("Survival Distribution in\nTesting Set")
gridExtra::grid.arrange(g1, g2, nrow = 1)
```

## Exercise 1: Try different splits and stratification!

Adjust the `prop=` argument and try different stratification variable for sampling. You can check the new distribution of `survived` by running the second chunk.

How would this impact your model?

Make sure to remove `eval=FALSE`

```{r,eval=FALSE}
titanic_split_2 <- initial_split(titanic, prop = , strata = )

titanic_split_2

titanic_test_2 <- testing(titanic_split_2)

titanic_train_2 <- training(titanic_split_2)
```

```{r,eval=FALSE}
g1 <- ggplot(titanic_train_2, aes(survived)) +
  geom_bar() +
  theme_minimal() +
  ggtitle("Survival Distribution in\nTraining Set")
g2 <- ggplot(titanic_test_2, aes(survived)) +
  geom_bar() +
  theme_minimal() +
  ggtitle("Survival Distribution in\nTesting Set")
gridExtra::grid.arrange(g1, g2, nrow = 1)

```

## **Which Models can be used with Classification Data?**

Remember "engine" in `tidymodels` defines which package or system is used to fit the model type. We can use `show_engines` to check which engines are available for a model type and what they can be used for.

```{r}
show_engines("rand_forest")
show_engines("logistic_reg")
```

## **Logistic regression recipe**

Let's define how the data should be preprocessed.

Using the training data, create and store a recipe setting `survived` as the outcome and using the following predictors. To learn more about recipes, you can check out our last week's workshop.

-   `pclass`: ticket class
-   `sex`
-   `age`
-   `sib_sp`: number of siblings or spouses aboard
-   `parch`: number of parents or children aboard
-   `fare`: passenger fare

Recall that there were missing values for `age`. To deal with this, add an imputation step using `step_impute_linear()`.

```{r}
logistic_recipe <- recipe(
  survived~pclass+sex+age+sib_sp+parch+fare,
  data = titanic_train) %>%
  step_impute_linear(age)
logistic_recipe
```

Next, use `step_dummy()` to **dummy** encode categorical predictors of class and sex.

```{r}
logistic_recipe <- logistic_recipe %>%
  step_dummy(pclass, sex)
logistic_recipe
```

How would you add an interaction with age and fare as well as sex and fare using `step_interact` and standardize using `step_normalize`?

```{r}
logistic_recipe <- logistic_recipe %>%
  step_interact(~ age:fare + starts_with("sex"):fare) %>%
  step_normalize(all_predictors())
logistic_recipe
```

**NOTE**: Why is it `starts_with("sex")`? This is what's in the documentation for `terms=` argument:

> consider using `tidyselect::starts_with()` when dummy variables have been created.

When dummy variables are created, new columns are created as `<variable_name>_<category>` and the original variable is deleted. Therefore, specifying interaction with the original variable name may not work.

## **Exercise 2: Define recipe at once**

I have added and illustrated each step in the recipe, but you can define all the steps at once by combining all the code. Try doing that yourself!

As a quick review, steps we defined are:

-   set model formula using `titanic_train` data

-   impute age by linear regression model

-   one-hot dummy code `pclass`, `sex`

-   add interaction between age vs fare and sex vs fare

-   normalize all predictors

```{r}

```

## **Bake Both Recipes**

Suppose we'd like to try a different model: looks like we can do classification with a model called random forest!

```{r}
show_engines("rand_forest")
```

Let's also create a recipe for a random forest model. For this model, I will use something called **one-hot encoding** when dummy coding.

```{r}
random_forest_recipe <- recipe(
  survived ~ pclass + sex + age + sib_sp + parch + fare,
  data = titanic_train) %>%
  step_impute_linear(age) %>%
  step_dummy(pclass,sex,one_hot = TRUE) %>%
  step_interact(~age:fare + starts_with("sex"):fare) %>%
  step_normalize(all_predictors())
```

How does one hot change your variables?

```{r}
logistic_recipe %>%
  prep() %>%
  bake(new_data=NULL) %>%
  select(starts_with("pclass"), starts_with("sex")) %>%
  head

random_forest_recipe %>%
  prep() %>%
  bake(new_data=NULL) %>%
  select(starts_with("pclass"), starts_with("sex")) %>%
  head
```

**NOTE**: Usually dummy variables take on value of 0 or 1, but we normalized all the predictors so they have mean 0 and standard deviation 1. Usually this wouldn't be the best practice, but it shouldn't impact the prediction output.

## **Fit your Logistic Model**

Now that we have the recipe for logistic regression model, we should initialize the logistic model for fitting. Logistic regression is usually done with the `glm` function. The appropriate engine to set for that is `"glm"`.

As we've seen before, some models can be used for both classification and regression. It's good practice to be explicit about which type of problem we are tackling, which in our case is classification.

```{r}
logistic_model <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

logistic_model
```

After model definition, we **create a workflow** that combines the recipe and the model.

```{r}
logistic_workflow <- workflow() %>%
  add_model(logistic_model) %>%
  add_recipe(logistic_recipe)

logistic_workflow
```

The only thing left to do is fit the model!

```{r}
logistic_fit <- logistic_workflow %>%
  fit(data=titanic_train)

# output model in a tidy format
tidy(logistic_fit)
```

## Fit a Random Forest With no Hyperparameters

Let's do the same thing for random forest model.

We'll fit two random forest models: one with a default set of hyperparameters and another one with custom hyperparameters. **Hyperparameters** are a set of parameters that define the model learning process.

-   `mtry`: number of predictors that is randomly samples for each split
-   `trees`: number of trees in a random forest model
-   `min_n`: minimum number of data points in a node that are required for it to be split further

```{r}
# initialize the model
random_forest_model<-rand_forest()%>%
  set_engine("ranger")%>%
  set_mode("classification")

# combine recipe with the model into one workflow
random_forest_workflow<-workflow()%>%
  add_model(random_forest_model)%>%
  add_recipe(random_forest_recipe)

# fit the final workflow
random_forest_fit<-random_forest_workflow%>%
  fit(data=titanic_train)
```

## **Fit a Second Random Forest with Hyper-parameters**

```{r}
# initialize the model
random_forest_model_2<-rand_forest(mtry = 8,trees = 1000,min_n = 4)%>%
  set_engine("ranger")%>%
  set_mode("classification")

# combine recipe with the model into one workflow
random_forest_workflow_2<-workflow()%>%
  add_model(random_forest_model_2)%>%
  add_recipe(random_forest_recipe)

# fit the final workflow
random_forest_fit_2<-random_forest_workflow_2%>%
  fit(data=titanic_train)
```

# **Which Model did Best?**

We'd like to know which model performs best on predicting survival outcome based on passenger information. To do this, we predict survival and evaluate performance using the testing dataset.

There are many different types of metrics available for evaluating classification. For this data, we will use accuracy, which is simply the proportion of the data that are predicted correctly. Other potential metrics are: Precision, Recall, and F1 Score.

```{r}
model_assessment1 <- logistic_fit %>%
  # make prediction on test data
  predict(new_data = titanic_test) %>%
  # bind prediction and the "true survival outcome" together
  bind_cols(titanic_test %>%
              select(survived)) %>%
  # calculate accuracy
  accuracy(truth = survived,estimate = .pred_class) %>%
  # add model name
  mutate(model="Logistic Regression")

model_assessment2<-random_forest_fit %>%
  predict(new_data=titanic_test) %>%
  bind_cols(titanic_test %>%
              select(survived)) %>%
  accuracy(truth=survived,estimate=.pred_class) %>%
  mutate(model="Random Forest Default Params")

model_assessment3<-random_forest_fit_2 %>%
  predict(new_data=titanic_test) %>%
  bind_cols(titanic_test %>%
              select(survived)) %>%
  accuracy(truth=survived,estimate=.pred_class) %>%
  mutate(model="Random Forest Custom Params")

model_assessment <- bind_rows(
  model_assessment1,
  model_assessment2,
  model_assessment3
)

print(model_assessment)
```

## Exercise 3: Other metrics

As I mentioned, there are other metrics available. Try using `precision()` and `recall()` in place of `accuracy()` and see if the same results hold!

```{r}
model_assessment1b <- logistic_fit %>%
  # make prediction on test data
  predict(new_data = titanic_test) %>%
  # bind prediction and the "true survival outcome" together
  bind_cols(titanic_test %>%
              select(survived)) %>%
  # calculate accuracy
  # accuracy(truth = survived,estimate = .pred_class) %>% # change your metric here
  # add model name
  mutate(model="Logistic Regression")

model_assessment2b <- random_forest_fit %>%
  predict(new_data=titanic_test) %>%
  bind_cols(titanic_test %>%
              select(survived)) %>%
  # accuracy(truth = survived,estimate = .pred_class) %>% # change your metric here
  mutate(model="Random Forest Default Params")

model_assessment3b <- random_forest_fit_2 %>%
  predict(new_data=titanic_test) %>%
  bind_cols(titanic_test %>%
              select(survived)) %>%
  # accuracy(truth = survived,estimate = .pred_class) %>% # change your metric here
  mutate(model="Random Forest Custom Params")

model_assessmentb <- bind_rows(
  model_assessment1b,
  model_assessment2b,
  model_assessment3b
)

print(model_assessmentb)
```

What are precision and recall? Here are the formulas for precision and recall:

$Precision = \frac{\# TruePositive}{\#TruePositive + \#FalsePositive}$

$Recall = \frac{\#TruePositive}{\#TruePositive+\#FalseNegative}$

These measures condition on different aspects of the data and the model output. In words, precision measures the proportion of "Positives" as predicted by the model to be actually positive, while recall measures the proportion of real positive cases identified as positive.

What would be the Recall if we have a model that labels everything as positive?

# Conclusion

In this workshop we reviewed the concepts we learned in workshop 1 and tried to use tidymodels for a different machine learning problem - classification. Once you get more familiar with the tidymodels framework, this package works for you to simplify the model building process. If you would like to learn more about tidymodels, I encourage you to check out guides available online!

-   <https://www.tidymodels.org/learn/>

-   <https://www.tidymodels.org/start/models/>

If you are more interested in using Python, `scikit-learn` is equivalent to `tidymodels`, which is covered in our (NSIP series)[https://github.com/nuitrcs/NextStepsInPython]

# **Bonus Section: Model tuning via grid search**

We would like to identify a set of hyperparameters that will give the best prediction.

But there are so many combinations of hyperparameters to choose from and it can be burdensome to create a workflow for each combination. This is where **grid search** comes into place. Grid search facilitates the process of computing performance metrics for a pre-defined set of hyperparameters. Let's try that with a random forest model!

First, in the model definition step, instead of specifying a hyperparameter with a specific value, we will pass `tune()`, which serves as a placeholder and lets the model know that we plan on tuning this parameter.

```{r}
random_forest_model_tune <- rand_forest(
  mtry = tune(), # tune this
  trees = 1000,
  min_n = tune() # and this
) %>%
  set_mode("classification") %>% 
  set_engine("ranger", importance = "impurity")

# define workflow as done before
random_forest_tune_workflow <- workflow() %>%
  add_model(random_forest_model_tune) %>%
  add_recipe(random_forest_recipe)
```

Next, we can define a set of hyperparameters we would like to try. There are convenience functions corresponding to the hyperparameters that we can use to define the grid.

There are various ways to choose values for the grid; in this example, we will choose uniformly across the range of possible values.

```{r}
random_forest_grid <- grid_regular(
  mtry(range = c(1,6)), # convenience function for mtry
  min_n(), # convenience function of min_n
  
  # number of values for each parameter we would like to try
  # below indicates 3 mtry values and 5 min_n values
  levels = c(mtry = 3, min_n = 3)
)

random_forest_grid
```

The grid is `tibble` with all combinations of the hyperparameters.

So far, we have a workflow that defines the preprocessing steps and the model, and a grid of hyperparameters we would like to test. How do we decide which set of hyperparameters is the best? Should we train on the training dataset and and choose the hyperparameters that can best predict the testing data? That is not a good idea because that will lead to what is called [overfitting](What is Overfitting? - Overfitting in Machine Learning Explained - AWS (amazon.com). Essentially, the model that seems to work well with our data will not generalize well to the real world and lead to overly optimistic result.

A better way to is to do **`resampling`**, in which we sample from our training set only repeatedly and test our hyperparameters - this way, the model is only based on the training set and the test set is only used for final model evaluation. The following image is a graphical representation of resampling.

![](images/resampling.svg)

For resampling of our data, we will use [**Cross Validation**](https://scikit-learn.org/stable/modules/cross_validation.html). We first define an object that holds information on how resampling is done.

```{r}
# Do 5-fold cross validation
cv_folds <- vfold_cv(titanic_train, v = 5)
```

We also need to decide which metrics is used to evaluate the performance. Let's use accuracy and precision. `metric_set()` combines multiple metric functions together into a new function that calculates all at once, so you're not restricted to using only one metric.

```{r}
random_forest_metrics <- metric_set(accuracy, precision)
```

Finally, we can also control some aspects of the grid search process.

```{r}
# do not print progress while fitting
# save the out of sample predictions in the output
ctrl <- control_grid(verbose = FALSE, save_pred = TRUE)
```

To recap, we have defined the following:

-   workflow that defines the preprocessing step and the model
-   resampling method
-   metrics to use for model evaluation
-   grid search process

All that is left is to train and get prediction result. `tune_grid()` takes all of this information and begins the grid search process.

```{r}
rf_tune_fit <- tune_grid(
  random_forest_tune_workflow,
  resamples = cv_folds,
  grid = random_forest_grid,
  metrics = random_forest_metrics,
  control = ctrl
)
rf_tune_fit
```

Each row in the output corresponds to 1 fold in cross validation. The data in `splits`, `.metrics`, and `.notes` are actually saved as a tibble (technically, as a list and there's a tibble inside). Let's check out what's saved in the first row in the `.metrics` column.

There are 9 x 2 = 18 rows. (9 hyperparameter combinations and 2 metrics)

```{r}
# first row in .metrics column
rf_tune_fit$.metrics[1]
```

This data structure seems complicated! How do we navigate through this and get information?

Luckily, there are functions to facilitate extracting and summarizing information from the grid search result.

### Summarize Metrics

This collects the accuracy and precision of the models across our cross validation and summarizes them as a mean and standard error.

```{r}
estimates <- collect_metrics(rf_tune_fit)
estimates
```

### Get Best Model

This shows us the best models in terms of the metrics we defined and gives us an idea of the types of hyperparameters that we might want to be using.

```{r}
# show top 5 best model based on accuracy
show_best(rf_tune_fit, metric = "accuracy")

# show best top 5model based on precision
show_best(rf_tune_fit, metric = "precision")

# find the tuning parameter combination with the best performance value
select_best(rf_tune_fit, metric = "accuracy")
```

### Get Predictions from Model Fitted with a Resample

This shows us how our models would predict our data. `id` column indicates the fold (iteration) of the resample.

```{r}
collect_predictions(rf_tune_fit) %>%
  head()
```

### Plot Results

This shows the performance of the different hyperparameters.

```{r}
autoplot(rf_tune_fit)
```

### Finalize workflow with the Best Hyperparameter Tuning

We can finalize our workflow once we have decided which model we would like to use.

```{r}
best_params <- select_best(rf_tune_fit, metric = "accuracy")

# finalize our random forest workflow and the fit to training data
tuned_model <- random_forest_tune_workflow %>% 
  finalize_workflow(best_params) %>%
  fit(data = titanic_train)
tuned_model
```

Let's finally evaluate our final model!

```{r}
tuned_assessment <- tuned_model %>%
  predict(new_data = titanic_test) %>%
  bind_cols(titanic_test %>% select(survived)) %>%
  accuracy(truth = survived, estimate = .pred_class) %>%
  mutate(model = "Random Forrest refined cross validation")

model_assessment %>%
  bind_rows(tuned_assessment) %>%
  arrange(-.estimate)
```

Thank you all for coming this week and feel free to stay around or email me if you have any questions!

# Exercise Solutions

## Exercise 1

```{r, eval=FALSE}
titanic_split_2 <- initial_split(titanic, prop = 0.95, strata = sex)

titanic_split_2

titanic_test_2 <- testing(titanic_split_2)

titanic_train_2 <- training(titanic_split_2)

g1 <- ggplot(titanic_train_2, aes(survived)) +
  geom_bar() +
  theme_minimal() +
  ggtitle("Survival Distribution in\nTraining Set")
g2 <- ggplot(titanic_test_2, aes(survived)) +
  geom_bar() +
  theme_minimal() +
  ggtitle("Survival Distribution in\nTesting Set")
gridExtra::grid.arrange(g1, g2, nrow = 1)
```

## Exercise 2

```{r, eval=FALSE}
logistic_recipe <- recipe(
  survived~pclass+sex+age+sib_sp+parch+fare,
  data = titanic_train) %>%
  step_impute_linear(age) %>%
  step_dummy(pclass, sex) %>%
  step_interact(~ age:fare + starts_with("sex"):fare) %>%
  step_normalize(all_predictors())
```

## Exercise 3

```{r, eval=FALSE}
model_assessment1b <- logistic_fit %>%
  # make prediction on test data
  predict(new_data = titanic_test) %>%
  # bind prediction and the "true survival outcome" together
  bind_cols(titanic_test %>%
              select(survived)) %>%
  # calculate accuracy
  precision(truth = survived,estimate = .pred_class) %>% # change your metric here
  # add model name
  mutate(model="Logistic Regression")

model_assessment2b <- random_forest_fit %>%
  predict(new_data=titanic_test) %>%
  bind_cols(titanic_test %>%
              select(survived)) %>%
  precision(truth = survived,estimate = .pred_class) %>% # change your metric here
  mutate(model="Random Forest Default Params")

model_assessment3b <- random_forest_fit_2 %>%
  predict(new_data=titanic_test) %>%
  bind_cols(titanic_test %>%
              select(survived)) %>%
  precision(truth = survived,estimate = .pred_class) %>% # change your metric here
  mutate(model="Random Forest Custom Params")

model_assessmentb <- bind_rows(
  model_assessment1b,
  model_assessment2b,
  model_assessment3b
)

print(model_assessmentb)

```
